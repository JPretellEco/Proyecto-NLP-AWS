# üé¨ SentimentScope: An√°lisis de Sentimientos con Transformers en Rese√±as de IMDB

## üß† Introducci√≥n al Proyecto

**SentimentScope** es un proyecto de *Deep Learning* desarrollado para **CineScope**, una empresa del sector del entretenimiento dedicada a ayudar a las audiencias a descubrir pel√≠culas y series que realmente disfrutar√°n.

El objetivo principal es construir y ajustar un modelo basado en **Transformers** capaz de comprender el sentimiento de los usuarios (positivo o negativo) a partir de rese√±as de pel√≠culas del conjunto de datos **IMDB**.
Este modelo busca mejorar los sistemas de recomendaci√≥n de CineScope mediante una comprensi√≥n m√°s profunda del lenguaje y una personalizaci√≥n m√°s precisa.

---

## üéØ Objetivos del Proyecto

* Desarrollar un **modelo Transformer** adaptado para la **clasificaci√≥n binaria de sentimientos**.
* Entrenar y evaluar el modelo con el **dataset de IMDB**, garantizando una alta precisi√≥n.
* Implementar una **arquitectura eficiente en PyTorch** con una canalizaci√≥n clara de entrenamiento, validaci√≥n y prueba.
* Contribuir al perfeccionamiento del motor de recomendaciones mediante insights derivados del an√°lisis de sentimientos.

---

## üß© Componentes Principales de SentimentScope

1. **Preparaci√≥n de Datos**

   * Limpieza, tokenizaci√≥n y divisi√≥n del conjunto de datos en *train*, *validation* y *test*.
   * Uso de *subword tokenization* basada en **BERT (uncased)** para equilibrar eficiencia y cobertura l√©xica.

2. **Arquitectura del Modelo Transformer**

   * Adaptaci√≥n de una arquitectura *Transformer Encoder* para tareas de clasificaci√≥n.
   * Inclusi√≥n de una **capa de pooling** para condensar las representaciones de tokens en un vector √∫nico.
   * A√±adido de una **capa densa final** que produce dos *logits*: positivo y negativo.

3. **Carga y Procesamiento de Datos**

   * Implementaci√≥n de un **DataLoader de PyTorch** para optimizar el flujo de datos durante el entrenamiento y evaluaci√≥n.

4. **Entrenamiento y Validaci√≥n**

   * Definici√≥n de funciones personalizadas para el c√°lculo de la p√©rdida (*loss function*) y la optimizaci√≥n (*optimizer*).
   * Entrenamiento mediante un enfoque por **√©pocas**, garantizando un aprendizaje progresivo y estable.

5. **Pruebas y Evaluaci√≥n**

   * Evaluaci√≥n del modelo sobre datos no vistos.
   * C√°lculo de m√©tricas de rendimiento como **accuracy**, **precision**, **recall** y **F1-score**.

---

## ‚öôÔ∏è Tecnolog√≠as Utilizadas

* **Lenguaje:** Python 3.x
* **Framework principal:** PyTorch
* **Transformers:** Biblioteca `transformers` de Hugging Face
* **Tokenizaci√≥n:** `BertTokenizer`
* **Procesamiento y an√°lisis:** NumPy, Pandas
* **Visualizaci√≥n:** Matplotlib, Seaborn
* **Dataset:** IMDB Reviews Dataset (50,000 rese√±as etiquetadas)

---

## üîç Diferencias Clave con Modelos Generativos

| Aspecto                 | Modelos Generativos                         | Modelo de Clasificaci√≥n (SentimentScope)      |
| ----------------------- | ------------------------------------------- | --------------------------------------------- |
| **Tarea principal**     | Predicci√≥n del siguiente token              | Predicci√≥n del sentimiento del texto completo |
| **Estructura de datos** | Secuencias continuas con ventana deslizante | Rese√±as independientes con etiquetas binarias |
| **Tokenizaci√≥n**        | A nivel de car√°cter                         | Subword (*BERT-based-uncased*)                |
| **Salida**              | Probabilidad por token                      | Vector de logits (positivo/negativo)          |
| **Entrenamiento**       | Muestreo continuo                           | Entrenamiento por √©pocas completas            |

---

## üß™ Flujo de Trabajo

1. **Carga del dataset IMDB**
2. **Preprocesamiento y tokenizaci√≥n**
3. **Configuraci√≥n del modelo Transformer**
4. **Definici√≥n de funciones de entrenamiento y validaci√≥n**
5. **Ejecuci√≥n del entrenamiento**
6. **Evaluaci√≥n sobre datos de prueba**
7. **An√°lisis de resultados y visualizaci√≥n de m√©tricas**

---

## üìä Resultados Esperados

* Precisi√≥n superior al **90%** en el conjunto de prueba.
* Curvas de p√©rdida y precisi√≥n estables a lo largo de las √©pocas.
* Interpretabilidad de resultados mediante *attention visualization* (opcional).

---

## üí° Aprendizajes Clave

* Comprensi√≥n de la adaptaci√≥n de Transformers para tareas de clasificaci√≥n.
* Dominio del flujo completo de **entrenamiento supervisado en PyTorch**.
* Manejo eficiente de grandes vol√∫menes de texto y optimizaci√≥n de *tokenizers*.

---

## üß∞ Pr√≥ximos Pasos

* Implementar t√©cnicas de **fine-tuning** con modelos preentrenados como *BERT-base-uncased*.
* Extender la clasificaci√≥n a m√∫ltiples sentimientos (*multi-class classification*).
* Integrar el modelo en una API REST con *FastAPI* o *Flask* para uso en producci√≥n.

---

## üë®‚Äçüíª Autor

**Jeffersson Hermano**
Estudiante de Econom√≠a, Ciencia de Datos y Big Data
Apasionado por la estad√≠stica, la programaci√≥n y la inteligencia artificial aplicada a problemas reales.

üìç *Per√∫*
üìß [Tu correo o LinkedIn opcional]

---

¬øQuieres que te agregue tambi√©n un bloque de ejemplo de c√≥digo (por ejemplo, c√≥mo instanciar el modelo y entrenarlo) al final del README para hacerlo m√°s completo? Puedo incluirlo con formato Markdown listo para GitHub.
